% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mr.ash.R
\name{mr.ash}
\alias{mr.ash}
\title{Multiple Regression with Adaptive Shrinkage}
\usage{
mr.ash(
  X,
  y,
  Z = NULL,
  sa2 = NULL,
  method = c("caisa", "sigma", "sigma_scaled", "sigma_indep", "accelerate", "block"),
  max.iter = 1000,
  min.iter = 1,
  beta.init = NULL,
  update.pi = TRUE,
  pi = NULL,
  update.sigma2 = TRUE,
  sigma2 = NULL,
  update.order = NULL,
  standardize = FALSE,
  intercept = TRUE,
  tol = set_default_tolerance()
)
}
\arguments{
\item{X}{The input matrix, of dimension (n,p); each column is a
single predictor; and each row is an observation vector. Here, n is
the number of samples and p is the number of predictors. The matrix
cannot be sparse.}

\item{y}{The observed quantitative responses, a vector of length p.}

\item{Z}{The covariate matrix, of dimension (n,k), where k is the
number of covariates.  The input matrix Z can be modified according
to "intercept" argument. If \code{Z = NULL} and \code{intercept =
TRUE}, then the actual \code{Z} will be the matrix having entries 1
of dimension (n,1). If \code{Z = NULL} and \code{intercept =
FALSE}, no intercept or covariates are inclued the model. If
\code{Z} is not \code{NULL} and \code{intercept = TRUE}, then the
intercept is added as a covariate to \code{Z}.}

\item{sa2}{The vector of mixture component variances. The variances
should be in increasing order, starting at zero; that is,
\code{sort(sa2)} should be the same as \code{sa2}. When \code{sa2 =
NULL}, the default setting is used, \code{sa2[k] = (2^(0.05*(k-1))
- 1)^2}, for \code{k = 1:20}. For this default setting,
\code{sa2[1] = 0}, and \code{sa2[20]} is roughly 1.}

\item{method}{\code{method = "caisa"}, an abbreviation of
"Cooridinate Ascent Iterative Shinkage Algorithm", fits the model
by approximate EM; it iteratively updates the variational
approximation to the posterior distribution of the regression
coefficients (the approximate E-step) and the model parameters
(mixture weights and residual covariance) in an approximate
M-step. Other settings of \code{method = "caisa"} are considered
experimental. In particular, \code{method = "block"} and
\code{method = "accelerate"} take different approaches to updating
the mixture weights; \code{method = "sigma_indep"}, \code{\method =
"sigma"} and \code{"sigma_scaled"}, take different approaches to
updating the residual variance \eqn{sigma^2}.}

\item{max.iter}{The maximum number of outer loop iterations allowed.}

\item{min.iter}{The minimum number of outer loop iterations allowed.}

\item{beta.init}{The initial estimate of the (approximate)
posterior mean regression coefficients. This should be \code{NULL},
or a vector of length p. When \code{beta.init = NULL}, the
posterior mean coefficients are all initially zero.}

\item{update.pi}{If \code{update.pi = TRUE}, the mixture
proportions in the mixture-of-normals prior are estimated from the
data. In the manuscript, \code{update.pi = TRUE}.}

\item{pi}{The initial estimate of the mixture proportions
\eqn{\pi_1,...,\pi_K}. If \code{pi = NULL}, the default value
\code{pi[k] = 1/K} for \code{k = 1:K} will be used.}

\item{update.sigma2}{If \code{update.sigma2 = TRUE}, the residual
variance \eqn{sigma^2} is estimated from the data.  In the manuscript,
\code{update.sigma = TRUE}.}

\item{sigma2}{The initial estimate of the residual variance,
\eqn{\sigma^2}. If \code{sigma2 = NULL}, the residual variance is
initialized to the empirical variance of the residuals based on the
initial estimates of the regression coefficients, \code{beta.init},
after removing linear effects of the intercept and any covariances.}

\item{update.order}{The order in which the co-ordinate ascent
updates for estimating the posterior mean coefficients are
performed. \code{update.order} can be \code{NULL}, \code{"random"},
or any permutation of \eqn{(1,...,p)}, where \code{p} is the number
of columns in the input matrix \code{X}. When \code{update.order}
is \code{NULL}, the co-ordinate ascent updates are performed in
order in which they appear in \code{X}; this is equivalent to
setting \code{update.order = 1:p}. When \code{update.order =
"random"}, the co-ordinate ascent updates are performed in a
randomly generated order, and this random ordering is different at
each outer-loop iteration.}

\item{standardize}{The logical flag for standardization of the
columns of X variable, prior to the model fitting. The coefficients
are always returned on the original scale.}

\item{intercept}{When \code{intercept = TRUE}, an intercept is
included in the regression model.}

\item{tol}{Additional settings controlling behaviour of the model
fitting algorithm. \code{tol$convtol} controls the termination
criterion for the model fitting. When \code{update.pi = TRUE}, the
outer-loop updates stop when the largest change in the mixture
weights is less than \code{convtol*K}; when \code{update.pi =
FALSE}, the outer-loop updates stop when the largest change in the
estimates of the posterior mean coefficients is less than
\code{convtol*K}. \code{tol$epstol} is a small, positive number
added to the likelihoods to avoid logarithms of zero.}
}
\value{
A list object with the following elements:

\item{intercept}{The estimated intercept.}

\item{beta}{A vector containing posterior mean estimates of the
  regression coefficients for all predictors.}

\item{sigma2}{The estimated residual variance.}

\item{pi}{A vector of containing the estimated mixture
  proportions}.

\item{iter}{The number of outer-loop iterations that were
  performed.}

\item{update.order}{The ordering used for performing the
  coordinate-wise updates. For \code{update.order = "random"}, the
  orderings for outer-loop iterations are provided in a vector of
  length \code{p*max.iter}, where }

\item{varobj}{A vector, with \code{length(varobj) = numiter},
  containing the value of the variational objective (equal to the
  negative of the "evidence lower bound") attained at each outer-loop
  iteration of the model fitting.}
}
\description{
Model fitting algorithms for Multiple Regression with
  Adaptive Shrinkage ("Mr.ASH"). Mr.ASH is a variational empirical
  Bayes (VEB) method for multiple linear regression. The fitting
  algorithms maximize the approximate marginal likelihood (the "evidence
  lower bound", or "ELBO") via coordinate-wise updates.
}
\details{
Mr.ASH adopts the following multiple linear regression
  model: \deqn{y | X, \beta, \sigma^2 \sim N(X \beta, \sigma^2 I_n),}
  in which the regression coefficients admit the following
  mixture-of-normals prior: \deqn{\beta | \pi, \sigma^2 ~
  \sum_{k=1}^K N(0, \sigma^2 \ sigma_k^2)}. Each mixture component is
  a normal density with zero mean and variance \eqn{\sigma^2
  \sigma_k^2}.

  The VEB approach solves the following optimization problem:
  \deqn{F(q,g,\sigma^2) = E_q \log p(y|X,\beta,\sigma^2) -
  \sum_{j=1}^p D_{KL}(q_j || g)} The algorithm updates the
  variational factors \eqn{q_1,...,q_p}, \eqn{g} and \eqn{\sigma^2}
  one at a time while fixing the others, in each outer loop
  iteration.

  See \sQuote{References} for more details about the VEB approach.
}
\examples{
### generate synthetic data
set.seed(1)
n           = 200
p           = 300
X           = matrix(rnorm(n*p),n,p)
beta        = double(p)
beta[1:10]  = 1:10
y           = X \%*\% beta + rnorm(n)

### fit Mr.ASH
fit.mr.ash  = mr.ash(X,y, method = "caisa")

### prediction routine
Xnew        = matrix(rnorm(n*p),n,p)
ynew        = Xnew \%*\% beta + rnorm(n)
ypred       = predict(fit.mr.ash, Xnew)

### test error
rmse        = norm(ynew - ypred, '2') / sqrt(n)

### coefficients
betahat     = predict(fit.mr.ash, type = "coefficients")
# this equals c(fit.mr.ash$intercept, fit.mr.ash$beta)

}
\references{
Y. Kim, W. Wang, P. Carbonetto, M. Stephens (2020). Fast and
flexible empirical Bayes approach to prediction in multiple
regression.
}
\seealso{
\code{\link{get.full.posterior}}

\code{\link{get.full.posterior}}
}
