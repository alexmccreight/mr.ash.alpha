% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mr.ash.R
\name{mr.ash}
\alias{mr.ash}
\title{Mr.ASH (Multiple Regression with Adaptive Shrikage)}
\usage{
mr.ash(
  X,
  y,
  Z = NULL,
  sa2 = NULL,
  method = c("caisa", "sigma", "accelerate", "block", "sigma_scaled", "sigma_indep"),
  max.iter = 1000,
  min.iter = 1,
  beta.init = NULL,
  update.pi = TRUE,
  pi = NULL,
  update.sigma2 = TRUE,
  sigma2 = NULL,
  update.order = NULL,
  standardize = FALSE,
  intercept = TRUE,
  tol = set_default_tolerance()
)
}
\arguments{
\item{X}{The input matrix, of dimension (n,p); each column is a single predictor;
and each row is an observation vector. Here n is the number of samples and
p is the number of predictors. Currently sparse matrix formats are not supported.}

\item{y}{The response variable. Currently we only allow the linear regression case
which corresponds to family = "gaussian" in glmnet package.
Thus we treat y as a real valued quantitative response variable.}

\item{Z}{The covariate matrix, of dimension (n,k); k is the number of covariates.
The input matrix Z can be modified according to "intercept" argument.
If \code{Z = NULL} and \code{intercept = TRUE}, then the actual \code{Z} will be
the matrix having entries 1 of dimension (n,1).
(\code{Z = NULL} by default, and if \code{intercept = FALSE}
then we do not include any covariates in the model.
If \code{intercept = TRUE}, then we will add the vector of ones to the columns of Z.
That is, \code{Z <- cbind(1,Z)}.)}

\item{sa2}{The vector of mixture component variances. Currently we only allow `sa2[1] = 0`.
for a technical reason. The default value is \code{sa2[k] = 2^(k-1) - 1}, for k = 1,...,20.

For Dev: (1) accelerate and block use different updates for g
(2) caisa, sigma, sigma_scaled, sigma_indep use different updates for sigma2,
based on different parametrizations
depending on whether sigma-dependent q, g and beta are used or not.
See reference for details.}

\item{method}{In the manuscript (preprint) listed in \sQuote{References}, only
\code{method = "caisa"} is used, which stands for Cooridinate Ascent Iterative Shinkage
Algorithm. Other method arguments will work, and produce similar outcomes unless the
regression setting is extreme.}

\item{max.iter}{The maximum number of outer loop iterations allowed.}

\item{min.iter}{The minimum number of inner loop iterations allowed.}

\item{beta.init}{The initial value for the variational posterior mean
of the regression coefficients.}

\item{update.pi}{The boolean parameter indicating whether the mixture proportion
\eqn{\pi} will be updated or not. In the manuscript, \code{update.pi = TRUE}.}

\item{pi}{The initial value for the mixture proportions \eqn{\pi_1,...,\pi_K}.
If \code{pi = NULL}, the default value \code{pi[k] = 1/K} for k = 1,...,K will be used.}

\item{update.sigma2}{The boolean parameter indicating whether the noise variance
\eqn{\sigma^2} will be updated or not. In the manuscript, \code{update.sigma = TRUE}.}

\item{sigma2}{The initial value for the noise variance \eqn{\sigma^2}.
If \code{sigma2 = NULL}, the default value \code{var(y-X\%*\%beta)} will be used.}

\item{standardize}{The logical flag for standardization of the columns of X variable,
prior to the model fitting. The coefficients are always returned on the original scale.}

\item{intercept}{The logical flag for including intercept (\code{intercept = TRUE})
to the model or not (\code{intercept = FALSE}).}

\item{tol}{The default tolerance is \code{epstol = 1e-12} and \code{convtol = 1e-8}.
See the documentation for \code{set_default_tolerance}. \code{epstol} stands for the
safeguard tolerance for mixture proportions (e.g. when \code{pi[1] * log(pi[1])} is
computed), and \code{convtol} stands for convergence tolerance.}
}
\value{
A list object with the following elements:

\item{intercept}{An intercept.}

\item{beta}{A vector of estimated regression coefficients (variational posterior means), 
after fixed effects (e.g. intercept) from covariates are subtracted out.}

\item{sigma2}{A scalar value of estimated noise variance (approximate maximum likelihood).}

\item{pi}{A vector of estimated mixture proportions of length K, where \code{K = length(sa2)}.}

\item{iter}{The number of total outer loop iterations implemented in the coordinate ascent algorithm.}

\item{varobj}{A sequence of variational objective values (which equals the negative evidence lower bound).
\code{length(varobj)} should be equal to \code{iter}.}

\item{data}{A preprocessed data used as the actual input for the algorithm. When \code{Z = NULL}
and \code{intercept = TRUE}, then the columns of X and y will be centered, and returned.
In general, Z will be regressed out, or equivalently, X and y will be projected into the space
orthogonal to Z, and then will be returned.}

\item{update.order}{An update order used for the outer loop iterations.}
}
\description{
The \code{mr.ash} function implements the Variational Empirical Bayes (VEB)
approach for prediction in multiple linear regression. It maximizes the approximate
marginal likelihood (a.k.a. evidence lower bound) using the coordinate ascent algorithm.
See \sQuote{References} for more details about the algorithms.
}
\details{
The VEB approach is based on the multiple linear regression model:
\deqn{y|X,\beta,\sigma^2 ~ N(X\beta, \sigma^2 I_n),  \beta | \pi, \sigma^2 ~ \sum_{k=1}^K N(0,\sigma^2\sigma_k^2)}
Here \eqn{\sigma_k^2} is the k-th mixture component variance \code{sa2[k]}
and \eqn{K} is the number of mixture components \code{length(sa2)}.
The other parameters are described in the \sQuote{Arguments}.

The VEB approach solves the following optimization problem:
\deqn{F(q,g,\sigma^2) = E_q \log p(y|X,\beta,\sigma^2) - \sum_{j=1}^p D_{KL}(q_j || g)}
The algorithm updates the variational factors \eqn{q_1,...,q_p}, \eqn{g} and \eqn{\sigma^2}
one at a time while fixing the others, in each outer loop iteration.

The algorithm does not store the full variational posterior \eqn{q = (q_1,...,q_p)},
but only stores the variational posterior mean \code{beta} for each regression coefficients.
In order to recover the full posterior, see the documnetation for \code{get.full.posterior} function.

See \sQuote{References} for more details about the VEB approach.
}
\examples{
### generate synthetic data
set.seed(1)
n           = 200
p           = 300
X           = matrix(rnorm(n*p),n,p)
beta        = double(p)
beta[1:10]  = 1:10
y           = X \%*\% beta + rnorm(n)

### fit Mr.ASH
fit.mr.ash  = mr.ash(X,y, method = "caisa")

### prediction routine
Xnew        = matrix(rnorm(n*p),n,p)
ynew        = Xnew \%*\% beta + rnorm(n)
ypred       = predict(fit.mr.ash, Xnew)

### test error
rmse        = norm(ynew - ypred, '2') / sqrt(n)

### coefficients
betahat     = predict(fit.mr.ash, type = "coefficients")
# this equals c(fit.mr.ash$intercept, fit.mr.ash$beta)

}
\references{
Y. Kim, W. Wang, P. Carbonetto, M. Stephens (2020),
Fast and Flexible Empirical Bayes Approach to Prediction in Multiple Regression.
}
\seealso{
The documentation for \code{get.full.posterior} function.
}
