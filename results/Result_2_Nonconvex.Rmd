---
title: "Result_2_NCVREG_and_MR.ASH"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This .Rmd file is to plot results for the experiment MR.ASH versus Non-convex Penalized Regression (NCVREG). The experiment is based on the following simulation setting.

#### Design setting

We sample the standard i.i.d. Gaussian measurement $X_{ij} \sim N(0,1)$ anda construct $X \in \mathbb{R}^p$ with $n = 500$ and $p = 2,000$.

#### Signal setting

We will use three different signal settings with different sparsity $s$.

(1) SparseNormal: $\beta_j \sim N(0,\sigma_\beta^2)$ for $j \in J$ and $\beta_j = 0$ otherwise, where $J$ is a set of randomly $s$ indices in $\{1,\cdots,p\}$, chosen uniformly at random.

(2) SparseT: $\beta_j \sim \textrm{t}_3$ for $j \in J$ and $\beta_j = 0$ otherwise, where $J$ is a set of randomly $s$ indices in $\{1,\cdots,p\}$, chosen uniformly at random.

(3) SparseLaplace: $\beta_j \sim \textrm{Laplace}(1)$ for $j \in J$ and $\beta_j = 0$ otherwise, where $J$ is a set of randomly $s$ indices in $\{1,\cdots,p\}$, chosen uniformly at random.

#### Sparsity

We vary $s = 5,10,20,40,80$.

#### PVE

Then we sample $y = X\beta + \epsilon$, where $\epsilon \sim N(0,\sigma^2 I_n)$.

We fix PVE = 0.99, where PVE is the proportion of variance explained, defined by

$$
{\rm PVE} = \frac{\textrm{Var}(X\beta)}{\textrm{Var}(X\beta) + \sigma^2},
$$
where $\textrm{Var}(a)$ denotes the sample variance of $a$ calculated using R function `var`. To this end, we set $\sigma^2 = \textrm{Var}(X\beta)$.

When PVE is very large, we expect that cross-validation (CV) and variational empirical Bayes (VEB) both can estimate a prior distribution of $\beta$ well. Therefore, one can compare directly how each method allows flexibility of prior distributions.

#### Non-convex Penalized Linear Regression

The `ncvreg` R package provides SCAD and MCP implementation. It seeks to minimize the following objective function.

$$
\frac{1}{2n} \| y - X\beta \|^2 + \textrm{pen}_{\lambda,\gamma}(\beta)
$$

$\lambda$ and $\gamma$ are tuning parameters. The R package `ncvreg` recommends for the default $\gamma$ value as follows.

(1) For MCP, the default $\gamma$ is $3$. Thus for MCP we use 11 equally spaced values from $1.1$ to $4.9$ including $3$ as a center point.

(2) For SCAD, the default $\gamma$ is $3.7$. Thus for SCAD we use 11 equally spaced vallues from $2.1$ to $5.3$ including $3.7$ as a center point.

We run `cv.ncvreg` with the default setting to tune $\lambda$ by cross-validation. Then we select a best tuple of $\lambda$ and $\gamma$ that minimizes the cross-validation error.

#### Packages / Libraries

```{r ggplot, message = FALSE}
library(ggplot2); library(cowplot);

gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}
```

## Results

#### Signal Shape 1: SparseNormal

Let us recall that we sample SparseNormal signal $\beta_j \sim N(0,\sigma_\beta^2)$ for $j \in J$ and $\beta_j = 0$ otherwise, where $J$ is a set of randomly $s$ indices in $\{1,\cdots,p\}$, chosen uniformly at random.

```{r fig1}
dat1 = readRDS("Experiment_2_Nonconvex_SparseNormal.RDS")
sdat = data.frame()
for (i in 1:5) {
  dmat       = matrix(dat1[[i]]$nrmse, 20, 5)
  nrmse      = colMeans(dmat)
  time       = colMeans(matrix(dat1[[i]]$t, 20, 5))
  sdat       = rbind(sdat, data.frame(nrmse = nrmse, time = time, s = 5 * 2^(i-1),
                                      fit = c("MR.ASH","SCAD","MCP","LASSO","L0Learn")))
}
sdat$fit = factor(sdat$fit, levels =  c("MR.ASH","SCAD","MCP","LASSO","L0Learn"))
p1 = ggplot(sdat) + geom_line(aes(x = s, y = nrmse, color = fit, linetype = fit)) +
  geom_point(aes(x = s, y = nrmse, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = 5 * 2^(0:4)) +
  labs(title = "Signal: SparseNormal", y = "predictior error (rmse / sigma)", x = "number of nonzero coefficients (s)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = gg_color_hue(5)[1:5])
p1
```

```{r fig2}
p2 = ggplot(sdat) + geom_bar(aes(x = factor(s), weight = nrmse, fill = fit),
                        position = "dodge2") +
  coord_cartesian(ylim = c(1.015,1.9)) +
  theme_cowplot(font_size = 14) +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "Signal: SparseNormal", y = "predictior error (rmse / sigma)", x = "number of coefficients (p)") +
  scale_fill_manual(values = gg_color_hue(5)[1:5])
p2
```

The above two figures display the prediction error. The prediction error we define here is

$$
\textrm{Pred.Err}(\hat\beta;y_{\rm test}, X_{\rm test}) = \frac{\textrm{RMSE}}{\sigma} = \frac{\|y_{\rm test} - X_{\rm test} \hat\beta \|}{\sqrt{n}\sigma}
$$
where $y_{\rm test}$ and $X_{\rm test}$ are test data sample in the same way. If $\hat\beta$ is fairly accurate, then we expect that $\rm RMSE$ is similar to $\sigma$. Therefore in average $\textrm{Pred.Err} \geq 1$ and the smaller the better.

The above 2 figures say that MR.ASH achieves the lowest prediction error in all cases.

```{r time1}
t1 = ggplot(sdat) + geom_line(aes(x = s, y = time, color = fit, linetype = fit)) +
  geom_point(aes(x = s, y = time, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = 100 * 2^(0:4)) +
  labs(title = "Signal: Gaussian", y = "computation time (sec)", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = gg_color_hue(5)[1:5])
t1
```

#### Signal Shape 2: SparseT

Let us recall that we sample SparseT signal $\beta_j \sim \textrm{t}_3$ for $j \in J$ and $\beta_j = 0$ otherwise, where $J$ is a set of randomly $s$ indices in $\{1,\cdots,p\}$, chosen uniformly at random.

```{r fig3}
dat1 = readRDS("Experiment_2_Nonconvex_SparseT.RDS")
sdat = data.frame()
for (i in 1:5) {
  dmat       = matrix(dat1[[i]]$nrmse, 20, 5)
  nrmse      = colMeans(dmat)
  time       = colMeans(matrix(dat1[[i]]$t, 20, 5))
  sdat       = rbind(sdat, data.frame(nrmse = nrmse, time = time, s = 5 * 2^(i-1),
                                      fit = c("MR.ASH","SCAD","MCP","LASSO","L0Learn")))
}
sdat$fit = factor(sdat$fit, levels =  c("MR.ASH","SCAD","MCP","LASSO","L0Learn"))
p3 = ggplot(sdat) + geom_line(aes(x = s, y = nrmse, color = fit, linetype = fit)) +
  geom_point(aes(x = s, y = nrmse, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = 5 * 2^(0:4)) +
  labs(title = "Signal: SparseT", y = "predictior error (rmse / sigma)", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = gg_color_hue(5)[1:5])
p3
```

```{r fig4}
p4 = ggplot(sdat) + geom_bar(aes(x = factor(s), weight = nrmse, fill = fit),
                        position = "dodge2") +
  coord_cartesian(ylim = c(1,2)) +
  theme_cowplot(font_size = 14) +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "Signal: SparseT", y = "predictior error (rmse / sigma)", x = "number of coefficients (p)") +
  scale_fill_manual(values = gg_color_hue(6)[1:5])
p4
```

The above 2 figures say that MR.ASH achieves the lowest prediction error in all cases.

```{r time2}
t2 = ggplot(sdat) + geom_line(aes(x = s, y = time, color = fit, linetype = fit)) +
  geom_point(aes(x = s, y = time, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = 100 * 2^(0:4)) +
  labs(title = "Signal: Gaussian", y = "computation time (sec)", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = gg_color_hue(5)[1:5])
t2
```

#### Signal 3: SparseLaplace

Let us recall that we sample SparseLaplace signal $\beta_j \sim \textrm{Laplace}(1)$ for $j \in J$ and $\beta_j = 0$ otherwise, where $J$ is a set of randomly $s$ indices in $\{1,\cdots,p\}$, chosen uniformly at random.

```{r fig5}
dat1 = readRDS("Experiment_2_Nonconvex_SparseLaplace.RDS")
sdat = data.frame()
for (i in 1:5) {
  dmat       = matrix(dat1[[i]]$nrmse, 20, 5)
  nrmse      = colMeans(dmat)
  time       = colMeans(matrix(dat1[[i]]$t, 20, 5))
  sdat       = rbind(sdat, data.frame(nrmse = nrmse, time = time, s = 5 * 2^(i-1),
                                      fit = c("MR.ASH","SCAD","MCP","LASSO","L0Learn")))
}
sdat$fit = factor(sdat$fit, levels =  c("MR.ASH","SCAD","MCP","LASSO","L0Learn"))
p5 = ggplot(sdat) + geom_line(aes(x = s, y = nrmse, color = fit, linetype = fit)) +
  geom_point(aes(x = s, y = nrmse, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = 5 * 2^(0:4)) +
  labs(title = "Signal: SparseLaplace", y = "predictior error (rmse / sigma)", x = "number of nonzero coefficients (s)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = gg_color_hue(5)[1:5])
p5
```

```{r fig6}
p6 = ggplot(sdat) + geom_bar(aes(x = factor(s), weight = nrmse, fill = fit),
                        position = "dodge2") +
  coord_cartesian(ylim = c(1.015,1.9)) +
  theme_cowplot(font_size = 14) +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "Signal: SparseLaplace", y = "predictior error (rmse / sigma)", x = "number of coefficients (p)") +
  scale_fill_manual(values = gg_color_hue(5)[1:5])
p6
```

```{r time3}
t3 = ggplot(sdat) + geom_line(aes(x = s, y = time, color = fit, linetype = fit)) +
  geom_point(aes(x = s, y = time, color = fit, shape = fit), size = 2.5) +
  theme_cowplot(font_size = 14) +
  scale_x_continuous(trans = "log10", breaks = 100 * 2^(0:4)) +
  labs(title = "Signal: Gaussian", y = "computation time (sec)", x = "number of coefficients (p)") +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(values = gg_color_hue(5)[1:5])
t3
```