---
title: "Experiment 10 (Variable selection)"
author: "Youngseok Kim"
date: "5/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(Matrix); library(Rcpp); library(varbvs); library(ggplot2); library(cowplot);
library(glmnet); library(susieR); library(BGLR); library(L0Learn); library(ncvreg)
sourceCpp("~/git/caisar/src/caisa_caisa.cpp");
sourceCpp("~/git/caisar/src/caisa_acceleration.cpp");
sourceCpp("~/git/caisar/src/caisa_update_g.cpp");
source("~/git/caisar/R/varmixopt.R");
source("~/git/caisar/R/misc.R");
source("~/git/caisar/R/etc.R");
```

```{r}
calc_fdr_power = function(nzind.fit, nzind){
  R = length(nzind.fit)
  V = sum(nzind.fit %in% nzind)
  s = length(nzind)
  return (c(fdr = 1 - V / max(R, 1), power = V / s))
}
```

```{r}
fdr1 = matrix(0,20,8)
power1 = matrix(0,20,8)
for (i in 1:20) {
  seed              = 2010 + i
  set.seed(seed)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  data$X           <- rnorm(dim(data$X)[1]) * sqrt(0.5) + data$X * sqrt(0.5)
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  
  train.index       = sample(n.total, floor(n.total * 0.5))
  X                 = data$X[train.index,]
  
  set.seed(seed)
  beta              = double(p)
  nzind             = which(sample(2, p, replace = TRUE, prob = c(0.05,0.95)) == 1)
  beta[nzind]       = rnorm(length(nzind)) * 2
  y.total           = data$X %*% beta + rnorm(n.total)
  
  y                 = y.total[train.index]
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  
  t.caisa           = system.time(
  fit.caisa        <- varmixopt(X = X, y = y, sa2 = sa2, max.iter = 2000, method = "caisa", 
                                stepsize = 1, tol = list(epstol = 1e-12, convtol = 1e-8)))
  t.susie           = system.time(
  fit.susie        <- susie(X = X, Y = y, standardize = TRUE))
  fit.susie$beta    = coef(fit.susie)[-1]
  t.lasso           = system.time(
  fit.lasso        <- cv.glmnet(x = X, y = y, standardize = TRUE))
  fit.lasso$beta    = coef(fit.lasso)[-1]
  t.enet = system.time(
  fit.enet         <- cv.glmnet(x = X, y = y, alpha = 0.9, standardize = TRUE))
  fit.enet$beta     = coef(fit.enet)[-1]
  t.bayesB          = system.time(
  fit.bayesB       <- BGLR(y, ETA = list(list(X = X, model="BayesB", standardize = TRUE)), verbose = FALSE))
  fit.bayesB$beta   = fit.blasso$ETA[[1]]$b * fit.bayesB$ETA[[1]]$d
  t.L0Learn         = system.time(
  fit.L0Learn      <- L0Learn.cvfit(X, y, nFolds=10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  fit.L0Learn$beta  = coef(fit.L0Learn, lambda = lambda.min)[-1]
  t.mcp             = system.time(
  fit.mcp          <- cv.ncvreg(X, y, penalty = "MCP", nfolds = 5))
  t.scad            = system.time(
  fit.scad         <- cv.ncvreg(X, y, penalty = "SCAD", nfolds = 5))
  
  nzind             = which(beta != 0)
  nzind.caisa       = which(get_phi(fit.caisa)[,1] < 0.5)
  nzind.susie       = which(fit.susie$pip > 0.5)
  nzind.lasso       = which(fit.lasso$beta != 0)
  nzind.enet        = which(fit.enet$beta != 0)
  nzind.bayesB      = which(fit.bayesB$ETA[[1]]$d > 0.5)
  nzind.L0Learn     = which(fit.L0Learn$beta != 0)
  nzind.mcp         = which(coef(fit.mcp)[-1] != 0)
  nzind.scad        = which(coef(fit.scad)[-1] != 0)
  
  res = cbind("caisa"  = calc_fdr_power(nzind.caisa, nzind),
              "susie"  = calc_fdr_power(nzind.susie, nzind),
              "lasso"  = calc_fdr_power(nzind.lasso, nzind),
              "nzind"  = calc_fdr_power(nzind.enet, nzind),
              "bayesB" = calc_fdr_power(nzind.bayesB, nzind),
              "L0Learn"= calc_fdr_power(nzind.L0Learn, nzind),
              "scad"   = calc_fdr_power(nzind.mcp, nzind),
              "mcp"    = calc_fdr_power(nzind.scad, nzind))
  fdr1[i,] = res[1,]
  power1[i,] = res[2,]
  cat(fdr1[i,],"\n")
  cat(power1[i,],"\n")
}
```

```{r}
fdr2 = matrix(0,20,8)
power2 = matrix(0,20,8)
for (i in 1:20) {
  seed              = 2010 + i
  set.seed(seed)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  data$X           <- rnorm(dim(data$X)[1]) * sqrt(0.5) + data$X * sqrt(0.5)
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  
  train.index       = sample(n.total, floor(n.total * 0.5))
  X                 = data$X[train.index,]
  
  set.seed(seed)
  beta              = double(p)
  beta[1:50]        = 10
  beta[51:200]      = 1
  y.total           = data$X %*% beta + rnorm(n.total)
  y                 = y.total[train.index]
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  
  t.caisa           = system.time(
  fit.caisa        <- varmixopt(X = X, y = y, sa2 = sa2, max.iter = 2000, method = "caisa", 
                                stepsize = 1, tol = list(epstol = 1e-12, convtol = 1e-8)))
  t.susie           = system.time(
  fit.susie        <- susie(X = X, Y = y, standardize = TRUE))
  fit.susie$beta    = coef(fit.susie)[-1]
  t.lasso           = system.time(
  fit.lasso        <- cv.glmnet(x = X, y = y, standardize = TRUE))
  fit.lasso$beta    = coef(fit.lasso)[-1]
  t.enet = system.time(
  fit.enet         <- cv.glmnet(x = X, y = y, alpha = 0.9, standardize = TRUE))
  fit.enet$beta     = coef(fit.enet)[-1]
  t.bayesB          = system.time(
  fit.bayesB       <- BGLR(y, ETA = list(list(X = X, model="BayesB", standardize = TRUE)), verbose = FALSE))
  fit.bayesB$beta   = fit.blasso$ETA[[1]]$b * fit.bayesB$ETA[[1]]$d
  t.L0Learn         = system.time(
  fit.L0Learn      <- L0Learn.cvfit(X, y, nFolds=10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  fit.L0Learn$beta  = coef(fit.L0Learn, lambda = lambda.min)[-1]
  t.mcp             = system.time(
  fit.mcp          <- cv.ncvreg(X, y, penalty = "MCP", nfolds = 5))
  t.scad            = system.time(
  fit.scad         <- cv.ncvreg(X, y, penalty = "SCAD", nfolds = 5))
  
  nzind             = which(beta != 0)
  nzind.caisa       = which(get_ash(fit.caisa)$result$lfsr < 0.1)
  nzind.susie       = which(fit.susie$pip > 0.5)
  nzind.lasso       = which(fit.lasso$beta != 0)
  nzind.enet        = which(fit.enet$beta != 0)
  nzind.bayesB      = which(fit.bayesB$ETA[[1]]$d > 0.5)
  nzind.L0Learn     = which(fit.L0Learn$beta != 0)
  nzind.mcp         = which(coef(fit.mcp)[-1] != 0)
  nzind.scad        = which(coef(fit.scad)[-1] != 0)
  
  res = cbind("caisa"  = calc_fdr_power(nzind.caisa, nzind),
              "susie"  = calc_fdr_power(nzind.susie, nzind),
              "lasso"  = calc_fdr_power(nzind.lasso, nzind),
              "nzind"  = calc_fdr_power(nzind.enet, nzind),
              "bayesB" = calc_fdr_power(nzind.bayesB, nzind),
              "L0Learn"= calc_fdr_power(nzind.L0Learn, nzind),
              "scad"   = calc_fdr_power(nzind.mcp, nzind),
              "mcp"    = calc_fdr_power(nzind.scad, nzind))
  fdr2[i,] = res[1,]
  power2[i,] = res[2,]
  cat(fdr2[i,],"\n")
  cat(power2[i,],"\n")
}
```

```{r}
fdr3 = matrix(0,20,8)
power3 = matrix(0,20,8)
for (i in 1:20) {
  set.seed(2010 + i)
  X                <- matrix(rnorm(1010 * 1000), 1010, 1000)
  p                 = 1000
  set.seed(seed)
  beta              = double(p)
  beta[1:100]       = rexp(100,.5) * sign(rnorm(100))
  y                <- X %*% beta + rnorm(1010)
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  
  t.caisa           = system.time(
  fit.caisa        <- varmixopt(X = X, y = y, sa2 = sa2, max.iter = 2000, method = "caisa", 
                                stepsize = 1, tol = list(epstol = 1e-12, convtol = 1e-8)))
  t.susie           = system.time(
  fit.susie        <- susie(X = X, Y = y, standardize = TRUE))
  fit.susie$beta    = coef(fit.susie)[-1]
  t.lasso           = system.time(
  fit.lasso        <- cv.glmnet(x = X, y = y, standardize = TRUE))
  fit.lasso$beta    = coef(fit.lasso)[-1]
  t.enet = system.time(
  fit.enet         <- cv.glmnet(x = X, y = y, alpha = 0.9, standardize = TRUE))
  fit.enet$beta     = coef(fit.enet)[-1]
  t.bayesB          = system.time(
  fit.bayesB       <- BGLR(y, ETA = list(list(X = X, model="BayesB", standardize = TRUE)), verbose = FALSE))
  fit.bayesB$beta   = fit.blasso$ETA[[1]]$b * fit.bayesB$ETA[[1]]$d
  t.L0Learn         = system.time(
  fit.L0Learn      <- L0Learn.cvfit(X, y, nFolds=10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  fit.L0Learn$beta  = coef(fit.L0Learn, lambda = lambda.min)[-1]
  t.mcp             = system.time(
  fit.mcp          <- cv.ncvreg(X, y, penalty = "MCP", nfolds = 5))
  t.scad            = system.time(
  fit.scad         <- cv.ncvreg(X, y, penalty = "SCAD", nfolds = 5))
  
  nzind             = which(beta != 0)
  nzind.caisa       = which(get_ash(fit.caisa)$result$lfsr < 0.1)
  nzind.susie       = which(fit.susie$pip > 0.5)
  nzind.lasso       = which(fit.lasso$beta != 0)
  nzind.enet        = which(fit.enet$beta != 0)
  nzind.bayesB      = which(fit.bayesB$ETA[[1]]$d > 0.5)
  nzind.L0Learn     = which(fit.L0Learn$beta != 0)
  nzind.mcp         = which(coef(fit.mcp)[-1] != 0)
  nzind.scad        = which(coef(fit.scad)[-1] != 0)
  
  res = cbind("caisa"  = calc_fdr_power(nzind.caisa, nzind),
              "susie"  = calc_fdr_power(nzind.susie, nzind),
              "lasso"  = calc_fdr_power(nzind.lasso, nzind),
              "nzind"  = calc_fdr_power(nzind.enet, nzind),
              "bayesB" = calc_fdr_power(nzind.bayesB, nzind),
              "L0Learn"= calc_fdr_power(nzind.L0Learn, nzind),
              "scad"   = calc_fdr_power(nzind.mcp, nzind),
              "mcp"    = calc_fdr_power(nzind.scad, nzind))
  fdr3[i,] = res[1,]
  power3[i,] = res[2,]
  cat(fdr3[i,],"\n")
  cat(power3[i,],"\n")
}
```

```{r}
df = data.frame(fdr1 = c(fdr1), power1 = c(power1),
                fdr2 = c(fdr2), power2 = c(power2),
                fdr3 = c(fdr3), power3 = c(power3),
                method = rep(c("CAISA","SUSIE","LASSO","ENET","BayesB","L0Learn","SCAD","MCP"), each = 20))
write.table(df, "dat121.txt", sep = ",")
```

```{r}
set.seed(2010 + 20)
  X                <- matrix(rnorm(1010 * 1000), 1010, 1000)
  p                 = 1000
  set.seed(2010 + 20)
  beta              = double(p)
  beta[1:500]       = seq(500,1,-1) / 50
  y                <- X %*% beta + rnorm(1010)
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  t.caisa           = system.time(
  fit.caisa        <- varmixopt(X = X, y = y, sa2 = sa2, max.iter = 2000, method = "caisa", 
                                stepsize = 1, tol = list(epstol = 1e-12, convtol = 1e-8)))
  a = get_ash(fit.caisa)
```


```{r}
phi = get_phi(fit.caisa)
#plot(phi[,1], pch = 20, cex = 1)
df = data.frame(x = 1:1000, y = phi[,1])
write.table(df, "dat124.txt", sep = ",")
```

```{r}
df = data.frame(x = 1:1000, y = get_ash(fit.caisa)$result$lfdr)
write.table(df, "dat122.txt", sep = ",")
p2 = ggplot() + geom_point(aes(x = 1:1000, y = get_ash(fit.caisa)$result$lfdr)) +
  theme_cowplot(font_size = 18) + 
  labs(x = "coefficient index",
       y = "local false discovery rate") + 
  theme(axis.line    = element_blank())
plot(a$result$lfsr, pch = 20, cex = 1, col = 3)
```

```{r}
a = c(seq(0,0.099,0.005),seq(0.1,0.89,0.01),seq(0.9,0.995,0.005))
b = posterior_quantile(fit.caisa,a)
c = colSums(b != 0)
a = c(a,1)
c = c(c,1000)
df = data.frame(x = a, y = c)
write.table(df, "dat123.txt", sep = ",")
p3 = ggplot() + geom_line(aes(x = a, y = c)) +
  theme_cowplot(font_size = 18) + 
  labs(x = "posterior quantile",
       y = "# of nonzero coefficients") + 
  theme(axis.line    = element_blank())
```

