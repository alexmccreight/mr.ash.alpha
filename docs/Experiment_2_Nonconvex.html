<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Youngseok Kim" />

<meta name="date" content="2019-10-01" />

<title>Experiment 2 (Nonconvex)</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">varbvs2</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Experiment 2 (Nonconvex)</h1>
<h4 class="author">Youngseok Kim</h4>
<h4 class="date">10/01/2019</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2019-09-28
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>varbvs2/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.4.0). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date </a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20190928code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20190928)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20190928code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20190928)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomstephenslabvarbvs2treea42c8544924b9eb0b59b58a22abe4915fc8d04f1targetblanka42c854a"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/stephenslab/varbvs2/tree/a42c8544924b9eb0b59b58a22abe4915fc8d04f1" target="_blank">a42c854</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomstephenslabvarbvs2treea42c8544924b9eb0b59b58a22abe4915fc8d04f1targetblanka42c854a" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rproj.user/

Untracked files:
    Untracked:  .DS_Store
    Untracked:  .Rbuildignore
    Untracked:  analysis/.DS_Store
    Untracked:  misc/
    Untracked:  paperfigures/fig21.pdf
    Untracked:  paperfigures/fig22.pdf
    Untracked:  paperfigures/fig23.pdf

Unstaged changes:
    Deleted:    backup/caisa_acc.cpp
    Deleted:    backup/caisa_em.cpp
    Deleted:    backup/caisa_order.cpp
    Deleted:    backup/g.cpp
    Deleted:    code/README.md
    Deleted:    outputs/dat811.txt
    Deleted:    outputs/dat812.txt
    Deleted:    outputs/dat813.txt
    Deleted:    outputs/dat821.txt
    Deleted:    outputs/dat822.txt
    Deleted:    outputs/dat823.txt
    Deleted:    outputs/dat831.txt
    Deleted:    outputs/dat832.txt
    Deleted:    outputs/pred811.txt
    Deleted:    outputs/pred812.txt
    Deleted:    outputs/pred813.txt
    Deleted:    outputs/pred821.txt
    Deleted:    outputs/pred822.txt
    Deleted:    outputs/pred823.txt
    Deleted:    outputs/pred831.txt
    Deleted:    outputs/pred832.txt
    Deleted:    outputs/pred833.txt
    Deleted:    outputs/time811.txt
    Deleted:    outputs/time812.txt
    Deleted:    outputs/time813.txt
    Deleted:    outputs/time821.txt
    Deleted:    outputs/time822.txt
    Deleted:    outputs/time823.txt
    Deleted:    outputs/time831.txt
    Deleted:    outputs/time832.txt
    Deleted:    outputs/time833.txt
    Modified:   paper/.DS_Store
    Modified:   paper/Experiment_2_Nonconvex.Rmd
    Modified:   paper/Experiment_7_RealGenotype.Rmd
    Modified:   paperfigures/fig51.pdf
    Modified:   paperfigures/fig52.pdf
    Modified:   paperfigures/fig53.pdf
    Modified:   paperfigures/fig61.pdf
    Modified:   paperfigures/fig62.pdf
    Modified:   paperfigures/fig63.pdf
    Modified:   paperfigures/fig81.pdf
    Modified:   paperfigures/fig82.pdf
    Modified:   paperfigures/fig83.pdf
    Modified:   paperresults/experiment2.rds
    Deleted:    results/.DS_Store
    Deleted:    results/.Rhistory
    Deleted:    results/dat101.txt
    Deleted:    results/dat102.txt
    Deleted:    results/dat103.txt
    Deleted:    results/dat11.txt
    Deleted:    results/dat111.txt
    Deleted:    results/dat112.txt
    Deleted:    results/dat113.txt
    Deleted:    results/dat12.txt
    Deleted:    results/dat121.txt
    Deleted:    results/dat122.txt
    Deleted:    results/dat123.txt
    Deleted:    results/dat124.txt
    Deleted:    results/dat13.txt
    Deleted:    results/dat131 copy.txt
    Deleted:    results/dat131.txt
    Deleted:    results/dat133.txt
    Deleted:    results/dat21.txt
    Deleted:    results/dat22.txt
    Deleted:    results/dat23.txt
    Deleted:    results/dat24.txt
    Deleted:    results/dat31.txt
    Deleted:    results/dat32.txt
    Deleted:    results/dat33.txt
    Deleted:    results/dat34.txt
    Deleted:    results/dat41.txt
    Deleted:    results/dat42.txt
    Deleted:    results/dat43.txt
    Deleted:    results/dat44.txt
    Deleted:    results/dat45.txt
    Deleted:    results/dat46.txt
    Deleted:    results/dat51.txt
    Deleted:    results/dat52.txt
    Deleted:    results/dat53.txt
    Deleted:    results/dat54.txt
    Deleted:    results/dat55.txt
    Deleted:    results/dat56.txt
    Deleted:    results/dat71.txt
    Deleted:    results/dat72.txt
    Deleted:    results/dat73.txt
    Deleted:    results/dat81.txt
    Deleted:    results/dat82.txt
    Deleted:    results/dat83.txt
    Deleted:    results/dat91.txt
    Deleted:    results/dat92.txt
    Deleted:    results/fig1.pdf
    Deleted:    results/fig10.pdf
    Deleted:    results/fig11.pdf
    Deleted:    results/fig12.pdf
    Deleted:    results/fig121.pdf
    Deleted:    results/fig122.pdf
    Deleted:    results/fig13.pdf
    Deleted:    results/fig2.pdf
    Deleted:    results/fig3.pdf
    Deleted:    results/fig31.pdf
    Deleted:    results/fig4.pdf
    Deleted:    results/fig41.pdf
    Deleted:    results/fig42.pdf
    Deleted:    results/fig7.pdf
    Deleted:    results/fig8.pdf
    Deleted:    results/fig9.pdf
    Deleted:    results/plots_for_paper.R

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the R Markdown and HTML files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view them.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/stephenslab/varbvs2/blob/a42c8544924b9eb0b59b58a22abe4915fc8d04f1/analysis/Experiment_2_Nonconvex.Rmd" target="_blank">a42c854</a>
</td>
<td>
Youngseok
</td>
<td>
2019-09-29
</td>
<td>
comparison with nonconvex reg
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This .Rmd file is to reproduce the result for a figure in the paper.</p>
<div id="goal" class="section level3">
<h3>Goal</h3>
<p>In what follows, we compare mr.ash with</p>
</div>
<div id="load-libraries-packages-and-codes" class="section level3">
<h3>Load libraries, packages and codes</h3>
<p>We load libraries, packages and codes for the simulation and for the plotting.</p>
<pre class="r"><code>library(ggplot2); library(cowplot); library(glmnet); library(ncvreg); library(L0Learn); library(varbvs2)</code></pre>
<pre><code>
********************************************************</code></pre>
<pre><code>Note: As of version 1.0.0, cowplot does not change the</code></pre>
<pre><code>  default ggplot2 theme anymore. To recover the previous</code></pre>
<pre><code>  behavior, execute:
  theme_set(theme_cowplot())</code></pre>
<pre><code>********************************************************</code></pre>
<pre><code>Loading required package: Matrix</code></pre>
<pre><code>Loading required package: foreach</code></pre>
<pre><code>Loaded glmnet 2.0-18</code></pre>
</div>
<div id="simulation-setting" class="section level3">
<h3>Simulation setting</h3>
<div id="design-matrix" class="section level4">
<h4>Design matrix</h4>
<p>We will use is an independent low dimensional Gaussian ensemble design. For abbreviation, we will call this design matrix “IndepLowdimGauss”. A matrix size <span class="math inline">\(n = 1010\)</span> and <span class="math inline">\(p = 1000\)</span> will be fixed. The design matrix will be generated as follows.</p>
<pre class="r"><code>standardize = FALSE
#X                &lt;- matrix(rnorm(500*1000), 500, 1000)
#X                &lt;- rnorm(dim(X)[1]) * sqrt(0.5) + X * sqrt(0.5)</code></pre>
</div>
</div>
</div>
<div id="simulation" class="section level2">
<h2>Simulation</h2>
<div id="signal-shape-1-sparsenormal" class="section level3">
<h3>Signal Shape 1 : SparseNormal</h3>
<div id="signal-sparsenormal9" class="section level4">
<h4>Signal = SparseNormal9</h4>
<pre class="r"><code>pred11 = matrix(0,20,5)
t11    = matrix(0,20,5)
null11 = double(20)
best11 = double(20)
sigma11 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,5)] = rnorm(5) * 2
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null11[i]         = norm(y.test, &#39;2&#39;)
  best11[i]         = norm(err.test, &#39;2&#39;)
  sigma11[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred11[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t11[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  mcp.lambda            = fit.mcp$lambda.min
  mcp.gamma             = 3.7
  scad.lambda           = fit.scad$lambda.min
  scad.gamma            = 3
  lasso.lambda          = fit.lasso$lambda.1se
  L0Learn.lambda        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  pi                    = c(fit.mrash$pi)
  sigma2                = fit.mrash$sigma2
  
  cat(pred11[i,],&quot;\n&quot;)
  cat(t11[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred12 = matrix(0,20,5)
t12    = matrix(0,20,5)
null12 = double(20)
best12 = double(20)
sigma12 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,10)] = rnorm(10) * 2
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null12[i]         = norm(y.test, &#39;2&#39;)
  best12[i]         = norm(err.test, &#39;2&#39;)
  sigma12[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred12[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t12[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred12[i,],&quot;\n&quot;)
  cat(t12[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred13 = matrix(0,20,5)
t13    = matrix(0,20,5)
null13 = double(20)
best13 = double(20)
sigma13 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,20)] = rnorm(20) * 2
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null13[i]         = norm(y.test, &#39;2&#39;)
  best13[i]         = norm(err.test, &#39;2&#39;)
  sigma13[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred13[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t13[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred13[i,],&quot;\n&quot;)
  cat(t13[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred14 = matrix(0,20,5)
t14    = matrix(0,20,5)
null14 = double(20)
best14 = double(20)
sigma14 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,40)] = rnorm(40) * 2
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null14[i]         = norm(y.test, &#39;2&#39;)
  best14[i]         = norm(err.test, &#39;2&#39;)
  sigma14[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred14[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t14[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred14[i,],&quot;\n&quot;)
  cat(t14[i,],&quot;\n&quot;)
}</code></pre>
</div>
<div id="signal-shape-threepointmass" class="section level4">
<h4>Signal Shape = ThreePointMass</h4>
<pre class="r"><code>pred21 = matrix(0,20,5)
t21    = matrix(0,20,5)
null21 = double(20)
best21 = double(20)
sigma21 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  ind               = sample(p,5)
  beta[ind[1:4]]    = 1
  beta[ind[5]]      = 10
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null21[i]         = norm(y.test, &#39;2&#39;)
  best21[i]         = norm(err.test, &#39;2&#39;)
  sigma21[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred21[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t21[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  mcp.lambda            = fit.mcp$lambda.min
  mcp.gamma             = 3.7
  scad.lambda           = fit.scad$lambda.min
  scad.gamma            = 3
  lasso.lambda          = fit.lasso$lambda.1se
  L0Learn.lambda        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  pi                    = c(fit.mrash$pi)
  sigma2                = fit.mrash$sigma2
  
  cat(pred21[i,],&quot;\n&quot;)
  cat(t21[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred22 = matrix(0,20,5)
t22    = matrix(0,20,5)
null22 = double(20)
best22 = double(20)
sigma22 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  ind               = sample(p,10)
  beta[ind[1:8]]   = 1
  beta[ind[9:10]]  = 10
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null22[i]         = norm(y.test, &#39;2&#39;)
  best22[i]         = norm(err.test, &#39;2&#39;)
  sigma22[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred22[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t22[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred22[i,],&quot;\n&quot;)
  cat(t22[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred23 = matrix(0,20,5)
t23    = matrix(0,20,5)
null23 = double(20)
best23 = double(20)
sigma23 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  ind               = sample(p,20)
  beta[ind[1:16]]   = 1
  beta[ind[17:20]]  = 10
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null23[i]         = norm(y.test, &#39;2&#39;)
  best23[i]         = norm(err.test, &#39;2&#39;)
  sigma23[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred23[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t23[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred23[i,],&quot;\n&quot;)
  cat(t23[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred24 = matrix(0,20,5)
t24    = matrix(0,20,5)
null24 = double(20)
best24 = double(20)
sigma24 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  ind               = sample(p,40)
  beta[ind[1:32]]   = 1
  beta[ind[33:40]]  = 10
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null24[i]         = norm(y.test, &#39;2&#39;)
  best24[i]         = norm(err.test, &#39;2&#39;)
  sigma24[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred24[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t24[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred24[i,],&quot;\n&quot;)
  cat(t24[i,],&quot;\n&quot;)
}</code></pre>
</div>
<div id="signal-shape-bimodal" class="section level4">
<h4>Signal Shape = Bimodal</h4>
<pre class="r"><code>pred31 = matrix(0,20,5)
t31    = matrix(0,20,5)
null31 = double(20)
best31 = double(20)
sigma31 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,5)] = rnorm(5,1,1)
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null31[i]         = norm(y.test, &#39;2&#39;)
  best31[i]         = norm(err.test, &#39;2&#39;)
  sigma31[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred31[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t31[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  mcp.lambda            = fit.mcp$lambda.min
  mcp.gamma             = 3.7
  scad.lambda           = fit.scad$lambda.min
  scad.gamma            = 3
  lasso.lambda          = fit.lasso$lambda.1se
  L0Learn.lambda        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  pi                    = c(fit.mrash$pi)
  sigma2                = fit.mrash$sigma2
  
  cat(pred31[i,],&quot;\n&quot;)
  cat(t31[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred32 = matrix(0,20,5)
t32    = matrix(0,20,5)
null32 = double(20)
best32 = double(20)
sigma32 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,10)] = rnorm(10,1,1)
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null32[i]         = norm(y.test, &#39;2&#39;)
  best32[i]         = norm(err.test, &#39;2&#39;)
  sigma32[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred32[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t32[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred32[i,],&quot;\n&quot;)
  cat(t32[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred33 = matrix(0,20,5)
t33    = matrix(0,20,5)
null33 = double(20)
best33 = double(20)
sigma33 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,20)] = rnorm(20,1,1)
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null33[i]         = norm(y.test, &#39;2&#39;)
  best33[i]         = norm(err.test, &#39;2&#39;)
  sigma33[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred33[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t33[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred33[i,],&quot;\n&quot;)
  cat(t33[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>pred34 = matrix(0,20,5)
t34    = matrix(0,20,5)
null34 = double(20)
best34 = double(20)
sigma34 = double(20)

for (i in 1:20) {
  set.seed(2010 + i)
  data              = list(X = matrix(rnorm(1000*2000), 1000, 2000))
  n.total           = dim(data$X)[1];
  p                 = dim(data$X)[2]
  train.index       = sample(n.total, floor(n.total * 0.5))
  test.index        = (1:n.total)[-train.index]
  X                 = data$X[train.index,]
  X.test            = data$X[test.index,]
  beta              = double(p)
  beta[sample(p,40)] = rnorm(40,1,1)
  sigma             = sqrt(sum(beta^2) / 99)
  y                &lt;- X %*% beta + sigma * rnorm(500)
  err.test          = sigma * rnorm(500)
  y.test           &lt;- X.test %*% beta + err.test
  sa2               = (sqrt(1.5)^(0:19) - 1)^2
  null34[i]         = norm(y.test, &#39;2&#39;)
  best34[i]         = norm(err.test, &#39;2&#39;)
  sigma34[i]        = sigma
  
  cat(&quot;pve =&quot;, mean((X %*% beta)^2) / mean(y^2),&quot;\n&quot;)
  
  t.scad            = system.time(
  fit.scad         &lt;- cv.ncvreg(X, y, penalty = &quot;SCAD&quot;, nfolds = 5))
    
  t.mcp             = system.time(
  fit.mcp          &lt;- cv.ncvreg(X, y, penalty = &quot;MCP&quot;, nfolds = 5))
    
  t.lasso           = system.time(
  fit.lasso        &lt;- cv.glmnet(x = X, y = y, alpha = 1, standardize = standardize))
    
  t.L0Learn         = system.time(
  fit.L0Learn      &lt;- L0Learn.cvfit(X, y, nFolds = 10))
  lambda.min        = fit.L0Learn$fit$lambda[[1]][which.min(fit.L0Learn$cvMeans[[1]])]
  
  t.mrash           = system.time(
  fit.mrash        &lt;- mr_ash(X = X, y = y, sa2 = sa2,
                             stepsize = 1, max.iter = 2000,
                             standardize = standardize,
                             tol = list(epstol = 1e-12, convtol = 1e-8)))
  
  pred34[i,] =  c(&quot;mrash&quot;      = norm(y.test - predict(fit.mrash, X.test), &#39;2&#39;),
                  &quot;scad&quot;       = norm(y.test - predict(fit.scad, X.test), &#39;2&#39;),
                  &quot;mcp&quot;        = norm(y.test - predict(fit.mcp, X.test), &#39;2&#39;),
                  &quot;lasso&quot;      = norm(y.test - predict(fit.lasso, X.test), &#39;2&#39;),
                  &quot;L0Learn&quot;    = norm(y.test - predict(fit.L0Learn, X.test, lambda = lambda.min)@x, &#39;2&#39;))
  t34[i,] =     c(&quot;mrash&quot;      = t.mrash[3],
                  &quot;scad&quot;       = t.scad[3],
                  &quot;mcp&quot;        = t.mcp[3],
                  &quot;lasso&quot;      = t.lasso[3],
                  &quot;t.L0Learn&quot;  = t.L0Learn[3])
  
  cat(pred34[i,],&quot;\n&quot;)
  cat(t34[i,],&quot;\n&quot;)
}</code></pre>
<pre class="r"><code>numlist = c(11,12,13,14,21,22,23,24,31,32,33,34)
dat = list()
for (i in 1:length(numlist)) {
  num = numlist[i]
  
  dat[[i]] = data.frame(pred = c(get(paste(&quot;pred&quot;, num, sep = &quot;&quot;))), time = c(get(paste(&quot;t&quot;, num, sep = &quot;&quot;))),
                        fit = rep(c(&quot;MR.ASH&quot;,&quot;SCAD&quot;,&quot;MCP&quot;,&quot;L0Learn&quot;,&quot;LASSO&quot;), each = 20))
  dat[[i]]$rmse = dat[[i]]$pred / sqrt(500)
  dat[[i]]$best = get(paste(&quot;best&quot;, num, sep = &quot;&quot;))
  dat[[i]]$null = get(paste(&quot;null&quot;, num, sep = &quot;&quot;))
  dat[[i]]$rrmse = (dat[[i]]$pred - dat[[i]]$best) / (dat[[i]]$null - dat[[i]]$best)
  dat[[i]]$rrmse2 = pmin(pmax(dat[[i]]$rrmse, 0),1)
  dat[[i]]$sigma = get(paste(&quot;sigma&quot;, num, sep = &quot;&quot;))
  dat[[i]]$p = 2000
}
saveRDS(dat, &quot;paperresults/experiment2.rds&quot;)</code></pre>
<pre class="r"><code>dat = readRDS(&quot;paperresults/experiment2.rds&quot;)
for (i in 1:12) {
  dat[[i]]$nrmse = dat[[i]]$rmse / dat[[i]]$sigma
}</code></pre>
<pre class="r"><code>sdat = list()
for (i in 1:4) {
  sdat[[i]] = data.frame(mean = colMeans(matrix(dat[[i]]$nrmse, nrow = 20)),
                         fit = c(&quot;MR.ASH&quot;,&quot;SCAD&quot;,&quot;MCP&quot;,&quot;L0Learn&quot;,&quot;LASSO&quot;),
                         nz = 5 * 2^(i-1),
                         ymin = apply(matrix(dat[[i]]$nrmse, nrow = 20), 2, function(x) quantile(x, probs = 0.25)),
                         ymax = apply(matrix(dat[[i]]$nrmse, nrow = 20), 2, function(x) quantile(x, probs = 0.75)))
}

df = rbind(sdat[[1]], sdat[[2]], sdat[[3]], sdat[[4]])
p1 = ggplot(df) + geom_line(aes(x = nz, y = mean, color = fit)) + theme_cowplot(font_size = 14) +
  #geom_errorbar(aes(x = nz, ymin = ymin, ymax = ymax, color = fit), width = .02, position=position_dodge(.01)) +
  scale_x_continuous(trans = &quot;log10&quot;, breaks = c(5,10,20,40)) +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  labs(x = &quot;number of nonzero coefficients (s)&quot;,
       y = &quot;prediction error (rmse/sigma)&quot;,
       title = &quot;SparseNormal&quot;)
ggsave(&quot;paperfigures/fig21.pdf&quot;, p1, width = 18, height = 6)
p1</code></pre>
<p><img src="figure/Experiment_2_Nonconvex.Rmd/fig1-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>sdat = list()
for (i in 1:4) {
  sdat[[i]] = data.frame(mean = colMeans(matrix(dat[[i+4]]$nrmse, nrow = 20)),
                         fit = c(&quot;MR.ASH&quot;,&quot;SCAD&quot;,&quot;MCP&quot;,&quot;L0Learn&quot;,&quot;LASSO&quot;),
                         nz = 5 * 2^(i-1),
                         ymin = apply(matrix(dat[[i+4]]$nrmse, nrow = 20), 2, function(x) quantile(x, probs = 0.25)),
                         ymax = apply(matrix(dat[[i+4]]$nrmse, nrow = 20), 2, function(x) quantile(x, probs = 0.75)))
}

df = rbind(sdat[[1]], sdat[[2]], sdat[[3]], sdat[[4]])
p2 = ggplot(df) + geom_line(aes(x = nz, y = mean, color = fit)) + theme_cowplot(font_size = 14) +
  #geom_errorbar(aes(x = nz, ymin = ymin, ymax = ymax, color = fit), width = .02, position=position_dodge(.05)) + 
  scale_x_continuous(trans = &quot;log10&quot;, breaks = c(5,10,20,40)) +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  labs(x = &quot;number of nonzero coefficients (s)&quot;,
       y = &quot;prediction error (rmse/sigma)&quot;,
       title = &quot;ThreePointMass&quot;)
ggsave(&quot;paperfigures/fig22.pdf&quot;, p2, width = 18, height = 6)
p2</code></pre>
<p><img src="figure/Experiment_2_Nonconvex.Rmd/fig2-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>sdat = list()
for (i in 1:4) {
  sdat[[i]] = data.frame(mean = colMeans(matrix(dat[[i+8]]$nrmse, nrow = 20)),
                         fit = c(&quot;MR.ASH&quot;,&quot;SCAD&quot;,&quot;MCP&quot;,&quot;L0Learn&quot;,&quot;LASSO&quot;),
                         nz = 5 * 2^(i-1),
                         ymin = apply(matrix(dat[[i+8]]$nrmse, nrow = 20), 2, function(x) quantile(x, probs = 0.25)),
                         ymax = apply(matrix(dat[[i+8]]$nrmse, nrow = 20), 2, function(x) quantile(x, probs = 0.75)))
}

df = rbind(sdat[[1]], sdat[[2]], sdat[[3]], sdat[[4]])
p3 = ggplot(df) + geom_line(aes(x = nz, y = mean, color = fit)) + theme_cowplot(font_size = 14) +
  #geom_errorbar(aes(x = nz, ymin = ymin, ymax = ymax, color = fit), width = .02, position=position_dodge(.05)) + 
  scale_x_continuous(trans = &quot;log10&quot;, breaks = c(5,10,20,40)) +
  theme(axis.line = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  labs(x = &quot;number of nonzero coefficients (s)&quot;,
       y = &quot;prediction error (rmse/sigma)&quot;,
       title = &quot;Bimodal&quot;)
ggsave(&quot;paperfigures/fig23.pdf&quot;, p3, width = 18, height = 6)
p3</code></pre>
<p><img src="figure/Experiment_2_Nonconvex.Rmd/fig3-1.png" width="672" style="display: block; margin: auto;" /></p>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.3 (2019-03-11)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14

Matrix products: default
BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] varbvs2_0.1-1 L0Learn_1.2.0 ncvreg_3.11-1 glmnet_2.0-18 foreach_1.4.4
[6] Matrix_1.2-17 cowplot_1.0.0 ggplot2_3.2.1

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.2       plyr_1.8.4       compiler_3.5.3   pillar_1.3.1    
 [5] git2r_0.26.1     workflowr_1.4.0  iterators_1.0.10 tools_3.5.3     
 [9] digest_0.6.18    evaluate_0.14    tibble_2.1.1     gtable_0.3.0    
[13] lattice_0.20-38  pkgconfig_2.0.2  rlang_0.3.4      yaml_2.2.0      
[17] xfun_0.9         withr_2.1.2      stringr_1.4.0    dplyr_0.8.1     
[21] knitr_1.24       fs_1.3.1         rprojroot_1.3-2  grid_3.5.3      
[25] tidyselect_0.2.5 glue_1.3.1       R6_2.4.0         rmarkdown_1.15  
[29] reshape2_1.4.3   purrr_0.3.2      magrittr_1.5     whisker_0.4     
[33] codetools_0.2-16 backports_1.1.4  scales_1.0.0     htmltools_0.3.6 
[37] assertthat_0.2.1 colorspace_1.4-1 labeling_0.3     stringi_1.4.3   
[41] lazyeval_0.2.2   munsell_0.5.0    crayon_1.3.4    </code></pre>
</div>
</div>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
